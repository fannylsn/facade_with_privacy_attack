[DATASET]
dataset_package = decentralizepy.datasets.RotatedCIFAR
dataset_class = RotatedCIFAR
model_class = LeNet
; provide directory containing "cifar-10-batches-py" folder | Pre-download recommended
; New download does not work with multiple processes | Crashes the first time, just retry
train_dir = ./eval/data/
test_dir = ./eval/data/
; python list of fractions below
; sizes = [[1/18]*12, [1/18]*6]
sizes = [[1/32]*16, [1/32]*16]
random_seed = 112233
validation_source = Train
; Train or Test set used to extract the validation set only on CIFAR-10 and FEMNIST
; On FEMNIST if the validation set is extracted from the test set is the same for all the clients
validation_size = 0.1
; fraction of the train or test set used as validation set, implemented only on CIFAR-10 and FEMNIST dataset
number_of_clusters = 2
number_of_models = 2
top_k_acc = 1

[NODE]
log_per_sample_loss = False
log_per_sample_pred_true = True
do_all_reduce_models = True
; share layer to boost mino
layers_sharing = True
share_all_for = 10
graph_package = decentralizepy.graphs.Regular
graph_class = Regular
graph_degree = 4
graph_seed = 1234

[OPTIMIZER_PARAMS]
optimizer_package = torch.optim
optimizer_class = SGD
lr = 0.01
; momentum = 0.9

[TRAIN_PARAMS]
training_package = decentralizepy.training.TrainingIDCA
training_class = TrainingIDCA
rounds = 10
; carefull, with full_epochs, rounds = epochs, else its the number of minibatch pass
full_epochs = False
batch_size = 8
shuffle = True
explore_models = False
loss_package = torch.nn
loss_class = CrossEntropyLoss

[COMMUNICATION]
comm_package = decentralizepy.communication.TCP
comm_class = TCP
addresses_filepath = /mnt/nfs/thiba/decentralizepy/tutorial/ip.json
offset = 6000

[SHARING]
sharing_package = decentralizepy.sharing.CurrentModelSharing
sharing_class = CurrentModelSharing